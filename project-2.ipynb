{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd396d2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "During the 2021 United Nations Climate Change Conference a report was published stating that 'Air pollution is the largest environmental risk to the public’s health, contributing to cardiovascular disease, lung cancer and respiratory diseases. It is costing the UK economy £20 billion a year and contributes to over 25,000 deaths a year' $^{1}$. However, this report could be masking a bigger issue of air polution worldwide. An OECD report published in 2016 states that air pollution costs 1% of GDP each year and accounts for 9 million premature deaths $^{2}$.\n",
    "\n",
    "With the human population growing at an exponential rate, it would seem, we now find ourselves in a continous loop. The Malthusian model, commonly used in development econmics, looks to model population growth. As a country grows food production, basic industry and infrastructure grow to service the population until an equilibrium level is reached. There are two takeaways from the model relating to air pollution. Firstly, air pollution does not cost and is in essence free to 'produce' implying there is no constraint on air pollution. Moreover, as less economically developed countries accelerate towards their equilibrium state, industry will have to keep up and air pollution is predicted to get worse.\n",
    "\n",
    "This being said, there appears to be other factors that affect air pollution. Global warming is well documented, however, the secondary and tertiary impacts on air pollution are less clear. Wildfires raged across Greece in August 2021, with the problem stemming from unusually hot and long summers. The Guardian reported that 'the fires were some of the worst on record' $^{3}$. Wildfires produce high amounts of black carbon, carbon dioxide, ozone precursors and black carbon into the atmosphere, causing air quality to plummet. \n",
    "\n",
    "However, with evolving winds, humidity levels and precipitation rates, areas once crippled by air pollution may soon see clearer skies while air pollution is involuntarily dumped onto other cities in a game of air pollution roulette. \n",
    "\n",
    "In this report we will look in depth at the factors above namely: demographics, weather and events. The report will aim to highlight areas of interest related to these factors in three countries: Mexico, UK and Iceland - representing an emerging nation and two developed nations with very different weather dynamics, industry and natural resources.  \n",
    "\n",
    "The report will focus upon 2 key measurements particulate matter 10 (pm10) and particulate matter 25 (pm25) these are defined as particulate matter that is smaller than 10 micrometres and 25 micrometres respecitivley. These were chosen given the breadth of activities that produce pm10 and pm25. The main contributor to pm10 and pm25 is the burning of oil, fuel and wood, however, pm25 and pm10 also include dust from industrial activity, landfill sites and even smaller bacteria. The wide range of polluting activities emitting these particulates will allow us to examine, explore and analyse the three nations in depth. \n",
    "\n",
    "Finally, we aim to predict air quality in a certain city for a certain day given the weather conditions using simple regression model.\n",
    "\n",
    "# Data & Measurements\n",
    "\n",
    "Most of the data in this report and prediction model comes from two apis. One for weather and one for pollution.\n",
    "We wrapped both of them in functions that call the apis, parse the response and return appropriate data frames.\n",
    "In addition to returning the data asked for, our functions also store the data in CSV files under ./data in the project directory. This is done to reduce network requests and also to be able to verify and play around with the data in raw format when exploring it. \n",
    "<br>If a csv file exists for the city the user asks for, and it contains all the rows for the queried dates, they are returned and the api is not called.\n",
    "\n",
    "We also have yearly population data. That is stored locally in csv files, no api there(something more plz).\n",
    "\n",
    "## Weather\n",
    "The source for weather data is the meteostat api found here https://dev.meteostat.net/python/. It has a convenient python library that allows the user to query for historical weather data for a particular location during a specified time interval.\n",
    "The python library even automatically collects data from different weather stations and bundles them together. \n",
    "For the purpose of this report, we need to transform the data a bit before using it. For example, we are only interested in a subset of all the data in the response. Furthermore, to ease usage of our model, our users should only provide a city name but the meteostat library only accepts latitude and longitude coordinates. \n",
    "To facilitate that, prior to using the meteostat api, we call a geocoding api from open-meteo.com [https://open-meteo.com/en/docs/geocoding-api]. It accepts a city name and returns all cities that match the name and their geographic latitude and longitude coordinates. \n",
    "\n",
    "The results from the open-meteo api are global. This means that if multiple cities share the same name, they are all in the API response. Because of this, we filter the results down to a particular country, and assume that if our user ask for 'London' she means London UK, not London New York USA. In addition, of all the cities that match a given country, we assume the user wants the most populated city.\n",
    "\n",
    "Armed with the latitude and longitude for the city the user wants to know about, we then finally call the meteostat api. \n",
    "As mentioned before, if we already have CSV data locally for the range, instead of going through all that was mentioned above,\n",
    "we just return appropriate data from the CSV file.\n",
    "## Pollution\n",
    "\n",
    "\n",
    "## Demography\n",
    "\n",
    "$^{1}$ http://www.local.gov.uk/parliament/briefings-and-responses/cop26-and-impact-air-pollution-public-health-and-wellbeing-house\n",
    "\n",
    "$^{2}$ OECD (2016), The Economic Consequences of Outdoor Air Pollution, OECD Publishing, Paris, https://doi.org/10.1787/9789264257474-en.\n",
    "\n",
    "$^{3}$ https://www.theguardian.com/world/2021/sep/30/its-like-a-war-greece-battles-increase-in-summer-wildfires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db5f8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pandas.core.frame import DataFrame\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import openaq\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from meteostat import Point\n",
    "from meteostat import Daily \n",
    "# Focus on these 5 cities and document their contry code\n",
    "cities_of_interest = {\"Akureyri\": \"IS\", \"London\": \"GB\", \n",
    "    \"Mexico City\": \"MX\", \"Newcastle\": \"GB\", \"Reykjavík\":\"IS\"}\n",
    "data_dirs = {\"weather\": \"data/weather\", \"pollution\": \"data/pollution\"}\n",
    "\n",
    "def fetch_weather_data(city_name, date_from=date.today() - timedelta(30), date_to=date.today()):\n",
    "    ''' Fetch weather measurements for a particular city during a particular time.\n",
    "\n",
    "    '''\n",
    "    #We limit to the 5 cities above\n",
    "    city_country = get_city_country(city_name)\n",
    "\n",
    "    #Since both the weather api and pandas dataframe operate on the datetime level\n",
    "    #cast date to datetime early \n",
    "    dt_from = cast_date_to_datetime(date_from)\n",
    "    dt_to = cast_date_to_datetime(date_to)\n",
    "\n",
    "    weather = fetch_data_from_csv('weather', city_country[0], dt_from= dt_from, dt_to=dt_to)\n",
    "\n",
    "    if(weather.empty):\n",
    "        weather = fetch_data_from_api('weather', city_country, dt_from, dt_to)\n",
    "    #The complicated case is: a file exists but not all the data is there\n",
    "    elif(needs_more_data(weather, dt_from, dt_to)):\n",
    "        weather = partial_csv_update('weather', weather, city_country)\n",
    "    return weather\n",
    "\n",
    "def fetch_pollution_data(city_name, date_from=date.today() - timedelta(days=30), date_to=date.today()):\n",
    "\n",
    "    city_country = get_city_country(city_name)\n",
    "\n",
    "    #Since both the weather api and pandas dataframe operate on the datetime level\n",
    "    #cast date to datetime early \n",
    "    dt_from = cast_date_to_datetime(date_from)\n",
    "    dt_to = cast_date_to_datetime(date_to)\n",
    "\n",
    "    pollution = fetch_data_from_csv('pollution', city_country[0], dt_from = dt_from, dt_to = dt_to)\n",
    "\n",
    "    if(pollution.empty):\n",
    "        pollution = fetch_data_from_api('pollution', city_country, dt_from, dt_to)\n",
    "    elif(needs_more_data(pollution, dt_from, dt_to)):\n",
    "        pollution = partial_csv_update('pollution', pollution, city_country)\n",
    "    return pollution \n",
    "\n",
    "def cast_date_to_datetime(date):\n",
    "    midnight_time = dt.min.time()\n",
    "    return dt.combine(date, midnight_time)\n",
    "\n",
    "def needs_more_data(data, dt_from, dt_to):\n",
    "    delta_days = (dt_to - dt_from).days + 1\n",
    "    data_subset = data[(dt_from <= data.index) & (data.index <= dt_to)]\n",
    "    return len(data_subset) < delta_days\n",
    "\n",
    "def get_city_country(city_name):\n",
    "    if city_name not in cities_of_interest.keys():\n",
    "        raise Exception(f\"Please enter one of {cities_of_interest}.\")\n",
    "    return (city_name, cities_of_interest[city_name])\n",
    "\n",
    "def fetch_data_from_csv(type, city_name, **kwargs):\n",
    "    file_dir = data_dirs[type]\n",
    "    \n",
    "    #Empty dataframe for when there is no csv file\n",
    "    data = pd.DataFrame({'' : []})\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(f'{file_dir}/{city_name}.csv', index_col='date', parse_dates=['date'])\n",
    "        if (kwargs):\n",
    "            return data[(kwargs['dt_from'] <= data.index) & (data.index <= kwargs['dt_to'])]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No historical {type} data found locally. Using API to get fresh data.\\n\")\n",
    "    return data\n",
    "\n",
    "def fetch_data_from_api(type, city_and_country, date_from, date_to):\n",
    "    # The first part of the tuple is the city and our csv data is organized by city name.\n",
    "    city_coords = query_lat_long(city_and_country)\n",
    "\n",
    "    if(type=='weather'):\n",
    "        data = query_historical_weather(city_coords, date_from, date_to)\n",
    "    elif(type=='pollution'):\n",
    "        data = query_historical_pollution(city_and_country, date_from, date_to)\n",
    "\n",
    "    #Write new csv file\n",
    "    dir_name = data_dirs[type]\n",
    "    data.to_csv(f'{dir_name}/{city_and_country[0]}.csv')\n",
    "    return data\n",
    "\n",
    "def partial_csv_update(type, from_api, city_and_country):\n",
    "    #Note that this is all of our CSV data for the city. no time filter\n",
    "    from_file = fetch_data_from_csv(type, city_and_country[0])\n",
    "\n",
    "    total_data = pd.concat([from_file, from_api])\n",
    "    unique_days = total_data.drop_duplicates()\n",
    "    unique_days = unique_days.sort_values('date')\n",
    "\n",
    "    dir_name = data_dirs[type]\n",
    "    unique_days.to_csv(f'{dir_name}/{city_and_country[0]}.csv')\n",
    "    return unique_days\n",
    "\n",
    "def filter_results_to_country(geoapi_response_data, country):\n",
    "    #If there are multiple cities with the same name, we choose the most populated\n",
    "    filter_by_country = [resp for resp in geoapi_response_data if resp['country_code'] == country]\n",
    "    # \n",
    "    sorted_by_pop = sorted(filter_by_country, key = lambda resp: resp['population'] if 'population' in resp else 0, reverse=True)\n",
    "    result = sorted_by_pop[0]\n",
    "\n",
    "    # Relevant subset of the result dict\n",
    "    return {key: result[key] for key in ('latitude', 'longitude', 'elevation', 'population')}\n",
    "\n",
    "def query_lat_long(city_country):\n",
    "    params_dict = {\n",
    "        'name': city_country[0],\n",
    "        #Default is 10 but since newcastle gives 9, we might hit the limit\n",
    "        'count': 100\n",
    "    }\n",
    "\n",
    "    # Since the user gives us more than 3 chars, the api performs fuzzy matching. So we do not\n",
    "    # need to worry abt spelling\n",
    "    resp = requests.get('https://geocoding-api.open-meteo.com/v1/search', params_dict)\n",
    "    data = resp.json()\n",
    "    \n",
    "    return filter_results_to_country(data['results'], city_country[1])\n",
    "\n",
    "def query_historical_weather(lat_long_elevation, date_from, date_to):\n",
    "    # Since we have dates and the api uses time, we need to convert from date to datetime\n",
    "    midnight_time = dt.min.time()\n",
    "    dt_from = dt.combine(date_from, midnight_time)\n",
    "    dt_to = dt.combine(date_to, midnight_time)\n",
    "    \n",
    "    location = Point(\n",
    "        lat_long_elevation['latitude'], \n",
    "        lat_long_elevation['longitude'], \n",
    "        lat_long_elevation['elevation'])\n",
    "\n",
    "    daily = Daily(location, start=dt_from, end=dt_to)\n",
    "    \n",
    "    # Ask meteostat to fill in any gaps in the data\n",
    "    daily.normalize()\n",
    "    data = daily.fetch()\n",
    "    \n",
    "    #tavg=Temp average (C).prcp=Total precipitation(mm). wdir=Wind direction(degrees)\n",
    "    #wspd=Average wind speed(km/h).wpgt=Wind peak gust(km/hr). pres=Sea-level air pressure(hpa)\n",
    "    #rhum=Relative humidity(does not work)\n",
    "    response = data[['tavg', 'prcp', 'wdir', 'wspd', 'wpgt', 'pres']]\n",
    "\n",
    "    #Since time is an index, simply calling rename does not work\n",
    "    tidy = response.rename_axis(index={\"time\": \"date\"})\n",
    "    #tidy['date'] = pd.to_datetime(tidy['date'], format='%y%m%d')\n",
    "\n",
    "    return tidy\n",
    "\n",
    "def get_weather(city_country):\n",
    "    coords = query_lat_long(city_country)\n",
    "    return query_historical_weather(coords)\n",
    "\n",
    "\n",
    "#From fetching measurements data notebook\n",
    "\n",
    "# TODO\n",
    "# 2. The dates are not the same for all cities.. Look into that\n",
    "# 3. Maybe count locations and provide that into the dataframe as well. \n",
    "#    It's relevent to know how many measures are in the city.\n",
    "# 4. Can we choose certain type of measures\n",
    "\n",
    "# Filter what city we want to get\n",
    "def filter_results_to_country(geoapi_response_data, country):\n",
    "    #If there are multiple cities with the same name, we choose the most populated\n",
    "    filter_by_country = [resp for resp in geoapi_response_data if resp['country_code'] == country]\n",
    "    # \n",
    "    sorted_by_pop = sorted(filter_by_country, key = lambda resp: resp['population'] if 'population' in resp else 0, reverse=True)\n",
    "    result = sorted_by_pop[0]\n",
    "\n",
    "    # Relevant subset of the result dict\n",
    "    return {key: result[key] for key in ('latitude', 'longitude', 'elevation', 'population')}\n",
    "\n",
    "def query_historical_pollution(city_country, date_from, date_to):\n",
    "    '''\n",
    "    This function takes in a city, parameter and date and writes data into a csv.file\n",
    "    Input:\n",
    "        city: name of a city (string)\n",
    "        ???parameter: List of strings that represent the parameters wanted to calculate\n",
    "        date_from: measurments after this date will be calculated\n",
    "        date_to: Measures until this date will be calculated\n",
    "    '''\n",
    "    \n",
    "    # Explicitly use v2 of the api http://dhhagan.github.io/py-openaq/api.html\n",
    "    api = openaq.OpenAQ(version ='v2')\n",
    "    # Get the longitude and latitude for the city\n",
    "    location = query_lat_long(city_country)\n",
    "    coords = f'{location[\"latitude\"]},{location[\"longitude\"]}'\n",
    "    \n",
    "    # Call the location api to check for the first date updated\n",
    "    locations = api.locations(coordinates = coords,radius = 10000,df = True)\n",
    "\n",
    "    min_date = locations[\"firstUpdated\"].min()\n",
    "    min_date = min_date.tz_convert(None)\n",
    "    min_date = pd.to_datetime(min_date) \n",
    "    \n",
    "    if min_date > date_from:\n",
    "        date_from = min_date  \n",
    "    \n",
    "    # Number of days we want measurements for\n",
    "    day_diff = (date_to - date_from).days\n",
    "    \n",
    "    # How we split the call between days to the API\n",
    "    split_days = 30\n",
    "\n",
    "    # Number of 30 day blocks in our range\n",
    "    number_months = day_diff // split_days\n",
    "\n",
    "    # Initialize the start date\n",
    "    start = date_from\n",
    "\n",
    "    # Add measurements data frame 30 days at a time\n",
    "    # An extra iteration for the remaining <30 days\n",
    "    for n in range(number_months + 1):\n",
    "        \n",
    "        # Find the end date\n",
    "        end = start + timedelta(days = split_days)\n",
    "        \n",
    "        # Fetch the data from the measurment api\n",
    "        try:\n",
    "            df_api = api.measurements(coordinates = coords, radius = 10000, df = True, \n",
    "                                        limit = 30000, parameter = [\"pm25\", \"pm10\"], value_from = 0,\n",
    "                                    date_from = start, date_to = end)\n",
    "        except KeyError:\n",
    "            print(f\"Opanaq API request with coordinates {coords} and date range {start}-{end} raised exception.\\n That can mean there is no data in that range.\")\n",
    "            return pd.DataFrame({'' : []})\n",
    "\n",
    "        # Start as the last end date\n",
    "        start = end\n",
    "\n",
    "        # For the first iteration create df\n",
    "        if n == 0: \n",
    "            df = df_api.copy()\n",
    "        # After the first iteration append the data\n",
    "        else:\n",
    "            df = df.append(df_api)\n",
    "    \n",
    "    ## Data prepping \n",
    "\n",
    "    # Change the index\n",
    "    df.index.name = 'Date.local'\n",
    "    df.reset_index(inplace=True)\n",
    "    df['date'] = df['Date.local'].dt.strftime('%Y-%m-%d')\n",
    "    df['value'] = df['value'].astype(float, errors = 'raise')\n",
    "\n",
    "    # Calculate mean, max and min value for each date\n",
    "    Result_mean = df.groupby(['date', 'parameter'],as_index=False)['value'].mean()\n",
    "    Result_max = df.groupby(['date', 'parameter'],as_index=False)['value'].max()\n",
    "    Result_min = df.groupby(['date', 'parameter'],as_index=False)['value'].min()\n",
    "\n",
    "    # Pivot the tables to wide format\n",
    "    ResultWide_mean = Result_mean.pivot_table(index='date',columns='parameter', values='value')\n",
    "    ResultWide_max = Result_max.pivot_table(index='date',columns='parameter', values='value')\n",
    "    ResultWide_min = Result_min.pivot_table(index='date',columns='parameter', values='value')\n",
    "\n",
    "    # Rename the columns to distinguish\n",
    "    ResultWide_mean.rename(columns={\"pm10\": 'pm10_mean', 'pm25': 'pm25_mean'}, inplace=True)\n",
    "    ResultWide_max.rename(columns={\"pm10\": 'pm10_max', 'pm25': 'pm25_max'}, inplace=True)\n",
    "    ResultWide_min.rename(columns={\"pm10\": 'pm10_min', 'pm25': 'pm25_min'}, inplace=True)\n",
    "\n",
    "    # Join mean and max first\n",
    "    df_first_join = pd.merge(ResultWide_mean, ResultWide_max, left_index=True, right_index=True)\n",
    "\n",
    "    # Join now to min\n",
    "    ResultWide = pd.merge(df_first_join, ResultWide_min, left_index=True, right_index=True)\n",
    "\n",
    "    # Change the index\n",
    "    ResultWide.index.name = 'date'\n",
    "    ResultWide.reset_index(inplace=True)\n",
    "\n",
    "    return ResultWide \n",
    "    \n",
    "# # Call the function\n",
    "city = 'Reykjavík'\n",
    "date_from = dt.strptime('01012020', \"%d%m%Y\").date()\n",
    "date_to = dt.strptime('01122021', \"%d%m%Y\").date()\n",
    "weather_data = fetch_weather_data(city)\n",
    "#pollution_data = fetch_pollution_data(city, date_from, date_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d57308",
   "metadata": {},
   "source": [
    "# Pollution by population\n",
    "\n",
    "Something something and charts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
